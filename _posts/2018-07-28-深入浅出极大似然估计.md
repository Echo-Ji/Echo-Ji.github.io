---
layout:     post
title:      深入浅出极大似然估计
subtitle:   学习EM算法前的补习
date:       2019-7-28
author:     Echo
header-style: text
catalog:    true
tags:
    - MLE
    - EM算法
---



> 由于EM算法和极大似然估计（Maximum likelihood estimation，简称MLE）有着千丝万缕的联系，所以我们先帮大家复习一下极大似然估计。


## 1 似然函数
通俗地讲，极大似然估计就是利用已知的样本结果（数据）信息，反推最**具有可能（最大概率）**导致这些样本结果（数据）出现的模型参数值。

考虑下图，红色叉号表示数据点，这组数据上方有三个高斯分布。现在假设这组数据全部来自于同一个分布，那最有可能是哪一个分布呢？我们都知道，高斯分布的参数为 $\theta=\{\mu, \sigma\}​$，那么问题其实可以表述为：图中三个分布对应的三组参数里，哪组参数能够更好的解释数据？即哪组参数让这些数据样本出现的可能性最大？

![MLE](https://github.com/Echo-Ji/Echo-Ji.github.io/raw/master/img/2019-07-28/MLE.png)

假设数据点表述为 $\mathcal{X}=\{x_1, x_2, \cdots, x_n\}$，且每个数据点之间都满足独立同分布条件，其概率密度函数为 $P(X=x|\theta)​$，那么所有数据出现的概率就是
$$
\begin{equation}
\begin{split}
P(\mathcal{X}|\theta) = \prod_{i=1}^{n}P(x_i|\theta).
\end{split}
\end{equation}\tag{1}
$$
可以看到公式中数据 $\mathcal{X}$ 是已知的，所以 $P(\mathcal{X}|\theta)$ 是一个关于 $\theta$ 的函数，大家通常称其为**似然函数**。

## 2 极大似然
接下来就要寻找能够更好地解释这组数据的参数 $\theta$，即使得函数 $P(\mathcal{X}|\theta)$ 的值最大的 $\theta$ ，写作
$$
\begin{equation}
\hat\theta = \mathop{\arg\max}_{\theta} \prod_{i=1}^{n}P(x_i|\theta).
\end{equation}\tag{2}
$$
想要问为什么的读者请回到本章第二段“通俗地讲”部分，仔细阅读三次那段话。回到正题，要最大化一个函数的值，我们首先想到的一定是一阶导数为零。但是观察公式 (1)，对于连乘的函数求导并不是一件容易的事情，于是考虑取对数简化运算。

有些读者可能会问，为什么一定要取对数而不是别的函数呢？这里有两点考虑：一方面，对数函数能够保持原函数的增减性，所以取对数前后函数的极值点保持一致；另一方面，对数函数能够变乘法为加法，大大降低了运算难度。于是**对数似然函数**便产生了，如下，
$$
\begin{equation}
\begin{split}
\mathcal{L}(\theta|\mathcal{X}) &= \mathrm{log}P(\mathcal{X}|\theta)\\
		&= \mathrm{log}\prod_{i=1}^{n}P(x_i|\theta)\\
		&= \sum_{i=1}^{n}\mathrm{log}P(x_i|\theta).
\end{split}
\end{equation}\tag{3}
$$
注意这里的对数似然函数写成 $\mathcal{L}(\theta|\mathcal{X})​$ 而不是 $\mathcal{L}(\theta)​$，是因为待估计量（参数）$\theta​$ 是随着观测数据 $\mathcal{X}​$ 的变化而变化的。因此优化目标变为：
$$
\begin{equation}
\hat\theta = \mathop{\arg\max}_{\theta} \sum_{i=1}^{n}\mathrm{log}P(x_i|\theta).
\end{equation}\tag{4}
$$
接下来便是对多维参数求偏导（若是一维参数则直接求导），然后令一阶导数为零，最后一一解出参数的估计值即可。

## 3 举个例子
这里我们以高斯分布为例，进行参数的极大似然估计。

首先高斯分布的概率密度函数为
$$
\begin{equation}
p(x)= \frac{1}{\sqrt{2 \pi} \sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},
\end{equation}\tag{5}
$$
假设数据为 $X = (x_1,x_2, \cdots,x_n)^T, \quad x_i  \overset{\text{iid}}{\sim} \mathcal{N}(\mu,\sigma^2)$，可以得到对数似然函数
$$
\begin{equation}
\begin{split}
\mathcal{L}(\mu,\sigma|X) &= \sum_{i=1}^{n}\mathrm{log}P(x_i|\mu, \sigma)\\
		&= \sum_{i=1}^{n}{\mathrm{log}\frac{1}{\sqrt{2 \pi} \sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}}\\
		&= \sum_{i=1}^{n}{\mathrm{log}\frac{1}{\sqrt{2\pi}}+\mathrm{log}\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}}.
\end{split}
\end{equation}\tag{6}
$$
然后用 $\mathcal{L}(\mu,\sigma|X)$ 分别对参数 $\mu$ 和 $\sigma$ 求偏导并令其为零，因为比较容易，所以这里省略化简过程，有兴趣的读者可以自己推导一下，最后可以得到参数的估计值：
$$
\begin{equation}
\begin{split}
&\hat\mu_{MLE} = \frac{1}{n}\sum_{i=1}^{n}x_i\\
&\hat\sigma_{MLE}^2= \frac{1}{n}\sum_{i=1}^{n}(x_i-\mu_{MLE})^2.
\end{split}
\end{equation}\tag{7}
$$
不难发现，用极大似然估计的高斯分布$\hat\mu$为所有样本数据点的均值，$\hat\sigma^2$ 为所有样本数据点方差。

## 4 算法流程
最后，简单总结一下极大似然估计参数的过程：

* 根据概率密度函数写出似然函数
* 对似然函数取对数，整理表达式
* 对表达式求一阶导，令导数为零，得到似然方程
* 解似然方程，得到参数的估计值