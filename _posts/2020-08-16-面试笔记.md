---
layout:         post
title:          KNN 算法
subtitle:       面试知识点笔记
date:           2020-08-15
author:         Echo
header-style:   text
catalog:        true
tags: 
    - 面试
    - 机器学习
    - 算法
---

> 面试前整理的一些自己不熟悉的知识点，好想拥有一个硬盘一样的脑袋，可以不忘掉的那种。

### 逻辑回归

手推逻辑回归：

* Logit 函数写模型
* 极大似然求梯度
* 学习率更新参数

### KNN 算法

KNN 算法属于懒学习方法，基本上不学习，导致预测的速度比逻辑回归等算法慢。

KNN 算法三要素：
* k 值选取：交叉验证选取，过小会引起过拟合，过大则模型太简单。
* 距离的度量方式：一般为欧式距离，当然曼哈顿距离、闵式距离也可。
* 分类决策规则：一般为多数表决法。

k 值减小，模型复杂，容易过拟合，k值增大，模型简单。

蛮力算法实现 KNN 通常适用于少量样本的简单模型，否则就要借助 KD 树来求解了。

KD 树的建立：选择方差最大的特征，求其中位数对应的样本，以此为基准将剩下的样本划分为两类，以此类推，直到没有可以划分的样本。

KD 树的预测：用最邻近算法跑 k 次，就得到了 k 个最邻近样本，然后根据多数表决法得到预测值。

KNN 算法的主要优缺点[这里](https://www.cnblogs.com/pinard/p/6061661.html)说的比较详细。

### 朴素贝叶斯

朴素贝叶斯的核心算法就是**贝叶斯公式**：
$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$
那么怎么应用呢？换个形式就会很明朗：
$$
P(类别|特征) = \frac{P(特征|类别)P(类别)}{P(特征)}
$$

其基本假设是条件独立性，即
$$
\begin{aligned}
P(X=x|Y=c_k) = & P(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)}|Y=c_k) \\
= & \prod_{j=1}^{n}P(X^{j}=x^{j}|Y=c_k)
\end{aligned}
$$
这是一个较强的假设，由于这一假设，模型包含的条件概率数量从 $mxn$ 降到了 $m+n$，因此，学习与预测大为化简。

概率估计方法可以有极大似然估计或贝叶斯估计（当 lambda 为 0 时退化为极大似然估计）。

### SVM

SVM 是一种二分类模型，它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使其有别于感知机，SVM 还包括核技巧，使它成为实质上的非线性分类器。

SVM 的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的最优化问题。其基本思想是**求解能够正确划分训练数据集且几何间隔最大的分离超平面**。

（间隔对偶核技巧）
带约束的原问题 -> 不带约束的原问题（引入拉格朗日） -> 对偶问题（凸二次规划问题，约束是线性的、目标函数是二次的->强对偶问题）-> 无约束优化求导为零解最优解

原、对偶问题是强对偶关系 <=> 满足 KKT 条件，KKT 条件为
$$
\left\{
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \mathbb{w}} = 0, \frac{\partial \mathcal{L}}{\partial b} = 0, \frac{\partial \mathcal{L}}{\partial \lambda} = 0\\
{\lambda}_{i}(1-y_{i}(\mathbb{w}^Tx_i + b)) = 0\\
{\lambda}_{i} \ge 0\\
1-y_{i}(\mathbb{w}^Tx_i + b) \le 0
\end{aligned}
\right.
$$

于是根据 KKT 条件求出参数即可。

手推 SVM 三步走：

* 几何解释  -> 数学解释
* 带约束优化 -> 无约束优化
* 对偶问题 -> KKT 条件

### BN

BN 是归一化隐藏单元激活值并加速学习的方式。

其奏效的原因是：

* 归一化到 0 均值、1 方差，可以加速学习

* 减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其他层，这有助于加速整个网络的学习

* 往每个隐藏层的激活值上增加了噪音，具有轻微的正则化效果。


## 决策树

信息增益存在偏向于选择取值较多的特征的问题，可用信息增益比来修正。分别对应 ID3 算法和 C4.5 算法。

决策树的生成学习仅考虑对训练数据的拟合程度，不考虑模型的复杂成都，容易过拟合，而剪枝学习同时考虑两者。

CART 树是二叉树，由特征选择、树的生成、树的剪枝组成，可用于回归和分类。基尼系数作为特征选择指标。基尼系数越小，分类结果越好。CART 剪枝形成一个序列，相当于超参数的选择过程。

基尼系数和熵之半的曲线很接近，都可以近似的代表分类误差率。

**对于连续值的处理**：用区间划分对其进行离散化，选择信息熵最小的划分，因为希望划分之后的数据更纯净。





